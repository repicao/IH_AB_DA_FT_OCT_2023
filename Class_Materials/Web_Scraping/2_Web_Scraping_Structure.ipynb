{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZGwPAsWetXf"
   },
   "source": [
    "# Multipage scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcYRZDp7gt1R"
   },
   "source": [
    "We practiced web scraping when all the information is in a single table of a single page in a site. What happens when we want to scrape information from multiple pages?\n",
    "\n",
    "Go to https://www.imdb.com/search/title/ and enter the following parameters, leaving all other fields blank or with its default value:\n",
    "\n",
    "- Title Type: Feature film\n",
    "\n",
    "- Release date: From 1990 to 1992\n",
    "\n",
    "- User Rating: 7.5 to \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJDhpufOgt1W"
   },
   "source": [
    "The page you get should be familiar. There's a list with movies and each movie has its title, release year, crew, etc. You could inspect the page and build the code to collect the date.\n",
    "\n",
    "However, the results we obtained contain 631 movies, and each page only contains 50 of them (you can change the settings to obtain up to 250 movies/page, but that still won't make it till the end).\n",
    "\n",
    "The way to automatize web scraping in these cases is to look at the URLs The one we've obtained is the following:\n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
    "\n",
    "If you scroll down and click on \"Next\", the URL is now: https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
    "\n",
    "Click again on \"Next\" and here's the new URL: https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
    "\n",
    "The patterns are clear: our search options are in the parameters title_type, release_date and user_rating. Then, we have the start parameter, which jumps in intervals of 50, and the ref_ parameter, which takes the value of \"adv_nxt\".\n",
    "\n",
    "Let's do some requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8mg-WhaaJcK"
   },
   "outputs": [],
   "source": [
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-01-01&user_rating=7.5,\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-01-01&user_rating=7.5,&start=51&ref_=adv_nxt\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-01-01&user_rating=7.5,&start=101&ref_=adv_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1623672374349,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "rl7dLg91gt1W"
   },
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1623672387568,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "Sp4U85bVgt1X"
   },
   "outputs": [],
   "source": [
    "# 2. url: we start with the 'second' page\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1317,
     "status": "ok",
     "timestamp": 1623672392543,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "HdYLue7rgt1X",
    "outputId": "d4c7d454-58ee-4482-c0b1-a65b8d6eb3ff"
   },
   "outputs": [],
   "source": [
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1623672396631,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "S2VCk5NQgt1Y",
    "outputId": "b12104f0-bba7-4f38-a475-cb0c392f86fc"
   },
   "outputs": [],
   "source": [
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# 4.2. check that the html code looks like it should\n",
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYJgUbl7gt1Z"
   },
   "source": [
    "Now, we'll have to build a loop where we simply replace the 51 for all the other values (jumping by 50) up until the end of the results. For simplicity, we will build manually this list of values to iterate through:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1623672450931,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "deRc6Fm4gt1Z",
    "outputId": "9b6ad1f2-607c-40c2-b260-b17e2ee1ed4f"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-01-01&user_rating=7.5,\"\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1623673785318,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "gxtUhQQKa8EX"
   },
   "outputs": [],
   "source": [
    "#string = \"&start=51&ref_=adv_nxt\"\n",
    "#string = \"&start=\" + str(number) + \"&ref_=adv_nxt\"\n",
    "#[51,101,....]\n",
    "#for step in range(51,327,50)\n",
    "#for i in range(0,8):\n",
    "#    if ( i == 0):\n",
    "#        url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-01-01&user_rating=7.5,\"\n",
    "#    else:\n",
    "#        string = str(50*i + 1)\n",
    "#        url = base_url + string + \"&ref_=adv_nxt\"\n",
    "#    print(url)\n",
    "\n",
    "urls = [ base_url if ( i == 0) else base_url + str(50*i + 1) + \"&ref_=adv_nxt\" for i in range(0,8)]\n",
    "#i = 1, elem = 51\n",
    "#i = 2, elem = 101\n",
    "#i = 3, elem = 151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PHqYpn9gt1Z"
   },
   "source": [
    "Respectful scraping:\n",
    "\n",
    "Before starting with the actual scraping, though, there's something we need to note when sending massive, automated requests to websites: it's rude.\n",
    "\n",
    "We just have 13 of them, which is not too many, but it's still a good practice to let a few seconds pass in between requests. Some pages don't like being scraped and will block your IP if they detect it's sending automated requests. Others might have a small server for the traffic they handle, and sending too many requests might crash the site.\n",
    "\n",
    "The sleep module will help us with that. Here's how it works, waiting 2 seconds between each iteration in a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1623673642679,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "qz7mq22Qgt1a"
   },
   "outputs": [],
   "source": [
    "# To make it more \"human\", we can randomize the waiting time:\n",
    "from time import sleep\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnU9x5Dbgt1a"
   },
   "source": [
    "We will now scrape all the pages and store the response into a list - waiting a few seconds in between requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40882,
     "status": "ok",
     "timestamp": 1623673965361,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "yV4tIwbPgt1a",
    "outputId": "082575a4-c344-465c-f0c0-b8329b7136ad"
   },
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if ( response.status_code == 200 ):\n",
    "        print(\"Webpage successfully retrieved!\")\n",
    "        #scrape_website()\n",
    "        seconds_to_wait = randint(2,7)\n",
    "        print(\"Wating {} seconds before scraping the next website!\".format(seconds_to_wait))\n",
    "        sleep(seconds_to_wait)\n",
    "    else:\n",
    "        print(\"Unable to get the html code. Jumping to the next website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG60yYztgt1a"
   },
   "source": [
    "Note how if you print the object pages after running the code above, you'll just see the response code messages, but the html code is still accessible and you can parse it the same way we've always done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7X1BJhlgt1a",
    "outputId": "148019b3-f9e1-4038-e385-9ce096d22baa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3Lo8OGZgt1b"
   },
   "source": [
    "It's the moment to build the code that collects all the 631 movie titles and their synopsis in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nWcSE7vgt1b"
   },
   "source": [
    "#### titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQZbYF8igt1b",
    "outputId": "4c6b432c-497c-4489-8e4e-2ac4c68aacf1"
   },
   "outputs": [],
   "source": [
    "# Parse just the first page, for testing purposes\n",
    "\n",
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "\n",
    "# Trim the selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Yrtcnigt1b"
   },
   "source": [
    "#### synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kDw7_39gt1c",
    "outputId": "a0a8004d-4d9c-4bba-87d2-493a5053b74e"
   },
   "outputs": [],
   "source": [
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVGxt3j9gt1c",
    "outputId": "57280019-f35a-4748-e120-4b57be54a7ab"
   },
   "outputs": [],
   "source": [
    "# Trim the selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xopdi2Tgt1c"
   },
   "source": [
    "There are many approaches to do this. The one we'll follow is: \n",
    "\n",
    "- Loop through the pages we collected, parse them (\"create the soup\") and store the parsed pages in a list. \n",
    "\n",
    "- For each parsed page, select the \"blocks of HTML elements\" that contain all the information of each movie (the title, the synopsis and other stuff). \n",
    "\n",
    "- For each one of the \"blocks\" we collected in the previous step: \n",
    "\n",
    "    - Get the movie titles and store them in a list \n",
    "\n",
    "    - Get the synopsis and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0c88qB8gt1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp4aYVlSgt1c"
   },
   "source": [
    "#### Scraping presidents\n",
    "\n",
    "Our objective is to create a dataframe with information about the presidents of the United States. To do this, we will go through this steps:\n",
    "\n",
    "1. Scrape this [list of presidents of the United States](https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgCd10-Lgt1c",
    "outputId": "59eacd7b-df83-4210-f7bc-5730942a022e"
   },
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "\n",
    "\n",
    "\n",
    "# 2. find url and store it in a variable\n",
    "\n",
    "# 3. download html with a get request\n",
    "\n",
    "\n",
    "# 4.1. parse html (create the 'soup')\n",
    "\n",
    "# 4.2. check that the html code looks like it should\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbjVEBEdgt1d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEQ8E_Lrgt1d"
   },
   "source": [
    "2. Collect all the links to the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FSQc53Egt1d",
    "outputId": "0234ff2d-e286-4853-bc2c-ec1fcd6daa3b"
   },
   "outputs": [],
   "source": [
    "# we can access the links searching for the attribute \"href\"\n",
    "# in each element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5aAVHpcgt1d",
    "outputId": "46cd14d0-3345-45cf-c678-04b9a0159331"
   },
   "outputs": [],
   "source": [
    "# Now, we just assemble a new request to the link\n",
    "# send request\n",
    "\n",
    "\n",
    "# parse & store html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sSoN8Ljgt1e"
   },
   "source": [
    "3. Scrape the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJR2lARbgt1e"
   },
   "source": [
    "In this step we could very well store the whole wikipedia page for each president, or just the tiny, final pieces of information. Storing the boxes is a middle ground (we don't have too much noise but retain the flexibility of deciding later which specific elements to extract).\n",
    "\n",
    "When sending multiple requests, remember to be respectful by spacing the requests a few seconds from each other. We will also pring the success code to monitor that everything is going well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX-rw1s1gt1e",
    "outputId": "6522c7c1-6a10-4013-d048-4e19415f5f9f"
   },
   "outputs": [],
   "source": [
    "# 2. find url and store it in a variable\n",
    "\n",
    "\n",
    "    # send request\n",
    " \n",
    "   \n",
    "    # parse & store html\n",
    "    \n",
    "    # respectful nap:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1lG7-yAgt1e"
   },
   "source": [
    "4. Find and store information about each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu2PSlpLgt1f"
   },
   "source": [
    "We extracted the 'infoboxes': now it's time to exctract especific pieces of information from them. Let's test what can we get from single presidents and then assemble a loop for all of them - as usual.\n",
    "\n",
    "Here, we will use [the string argument](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-string-argument) in the find function, since wikipedia tags and classes are not always helpfulto locate. The string argument allows us to locate elements by its actual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS3fh0o7gt1f",
    "outputId": "67be1e56-27f3-4cca-ebdd-185a46a20398"
   },
   "outputs": [],
   "source": [
    "#Birthday\n",
    "\n",
    "#Political party\n",
    "\n",
    "#Number of sons/daughters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzizmsRogt1f"
   },
   "source": [
    "5. Organize the information in a dataframe where we have each president as a row and each variable we collected as a column. Consider: .json_normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh8ZY6Ovgt1f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Web scraping code along 2 - Structure.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
